Comparative Analysis of Large Language Models in Human-AI Conversations with Natural Language Processing

![EDA_summary_fig](https://github.com/user-attachments/assets/67f06224-df02-4739-9a1d-e3406070e071)

Large Language Models (LLMs) represent a transformative leap in artificial intelligence, underpinning the development of systems capable of understanding and generating human-like text across diverse applications. Recent advancements in LLMs, such as OpenAI’s GPT-4 and Anthropic’s Claude-v1, have showcased exceptional capabilities in reasoning, conversational fluency, and adapting to complex domains. These innovations are the result of advancements in transformer-based architectures and sophisticated pretraining techniques on vast datasets, enabling unprecedented generalization and adaptability (Ali Reza, et al., 2024). 
Despite their success, challenges persist in evaluating LLM performance beyond traditional benchmarks. Real-world tasks often demand nuanced understanding, contextual reasoning, and dynamic decision-making—qualities that are difficult to measure through static datasets (Mohaimenul Azam Khan, et al., 2024). This work leverages the Chatbot Arena dataset, comprising over 25,000 interactions between users and 20 LLMs, to bridge this gap. The study focuses on evaluating model performance in generating responses and identifying optimal models for different prompts based on quantitative metrics such as win rates and qualitative measures like prompt “hardness” (difficulty). These insights have the potential to inform the development of advanced conversational AI systems optimized for specific user needs or scenarios. 

Project Drive: https://drive.google.com/drive/folders/1wEJBEdrmVGwKNd_SoPeO2zeuopxUXNGR?usp=drive_link
